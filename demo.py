#!/usr/bin/env python3
"""
Orchestra æ¼”ç¤ºè„šæœ¬
å±•ç¤ºå¦‚ä½•ä½¿ç”¨Orchestraå®ç°è¿›è¡ŒåŸºæœ¬çš„æ— ç›‘ç£èšç±»ä»»åŠ¡
"""

import os
import sys
import torch
import numpy as np

# è®¾ç½®matplotlibåç«¯ï¼ˆè§£å†³Colabç¯å¢ƒé—®é¢˜ï¼‰
# æ¸…é™¤å¯èƒ½å†²çªçš„ç¯å¢ƒå˜é‡
if 'MPLBACKEND' in os.environ:
    del os.environ['MPLBACKEND']

import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt

from sklearn.datasets import make_blobs, make_circles, make_moons
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from orchestra_model import OrchestraModel, OrchestraLoss, OrchestraTrainer
from federated_orchestra import OrchestraConfig, OrchestraDataProcessor

def create_synthetic_datasets():
    """åˆ›å»ºåˆæˆæ•°æ®é›†ç”¨äºæ¼”ç¤º"""
    datasets = {}
    
    # 1. ç®€å•çš„é«˜æ–¯èšç±»
    X1, y1 = make_blobs(n_samples=800, centers=4, n_features=2, 
                       random_state=42, cluster_std=1.5)
    datasets['blobs'] = (X1, y1, 'é«˜æ–¯èšç±»')
    
    # 2. åœ†å½¢èšç±»
    X2, y2 = make_circles(n_samples=800, noise=0.1, factor=0.3, random_state=42)
    datasets['circles'] = (X2, y2, 'åœ†å½¢èšç±»')
    
    # 3. æœˆç‰™å½¢èšç±»
    X3, y3 = make_moons(n_samples=800, noise=0.1, random_state=42)
    datasets['moons'] = (X3, y3, 'æœˆç‰™å½¢èšç±»')
    
    # 4. é«˜ç»´æ•°æ®ï¼ˆæ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼‰
    X4, y4 = make_blobs(n_samples=1000, centers=5, n_features=50, 
                       random_state=42, cluster_std=2.0)
    datasets['high_dim'] = (X4, y4, 'é«˜ç»´èšç±»')
    
    return datasets

def visualize_2d_data(X, y_true, y_pred, title, save_path=None):
    """å¯è§†åŒ–2Dæ•°æ®çš„èšç±»ç»“æœ"""
    if X.shape[1] != 2:
        print(f"è·³è¿‡å¯è§†åŒ– {title}ï¼ˆä¸æ˜¯2Dæ•°æ®ï¼‰")
        return
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # çœŸå®æ ‡ç­¾
    scatter1 = ax1.scatter(X[:, 0], X[:, 1], c=y_true, cmap='tab10', alpha=0.7)
    ax1.set_title(f'{title} - çœŸå®æ ‡ç­¾')
    ax1.grid(True, alpha=0.3)
    
    # é¢„æµ‹æ ‡ç­¾
    scatter2 = ax2.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='tab10', alpha=0.7)
    ax2.set_title(f'{title} - é¢„æµ‹èšç±»')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {save_path}")
    
    plt.show()

def run_orchestra_clustering(X, y_true, dataset_name, config):
    """è¿è¡ŒOrchestraèšç±»"""
    print(f"\n{'='*60}")
    print(f"è¿è¡Œ {dataset_name} èšç±»")
    print(f"æ•°æ®å½¢çŠ¶: {X.shape}, çœŸå®èšç±»æ•°: {len(np.unique(y_true))}")
    print(f"{'='*60}")
    
    # æ•°æ®é¢„å¤„ç†
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # è½¬æ¢ä¸ºtensor
    X_tensor = torch.tensor(X_scaled, dtype=torch.float32)
    y_tensor = torch.tensor(y_true, dtype=torch.long)
    
    # åˆ›å»ºæ¨¡å‹
    model = OrchestraModel(
        input_dim=X.shape[1],
        hidden_dims=config.hidden_dims,
        embedding_dim=config.embedding_dim,
        num_clusters=config.num_clusters,
        dropout_rate=config.dropout_rate,
        temperature=config.temperature
    )
    
    # åˆ›å»ºæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    loss_fn = OrchestraLoss(
        contrastive_weight=config.contrastive_weight,
        clustering_weight=config.clustering_weight,
        consistency_weight=config.consistency_weight,
        temperature=config.temperature
    )
    
    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    trainer = OrchestraTrainer(model, loss_fn, optimizer, device)
    
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    
    # è®­ç»ƒ
    print("\nå¼€å§‹è®­ç»ƒ...")
    training_history = {
        'epoch': [],
        'total_loss': [],
        'contrastive_loss': [],
        'clustering_loss': [],
        'consistency_loss': [],
        'ari_score': [],
        'nmi_score': []
    }
    
    for epoch in range(config.num_epochs):
        # æ¨¡æ‹Ÿè”é‚¦å­¦ä¹ ï¼šå°†æ•°æ®åˆ†æˆä¸¤ä¸ªæ‰¹æ¬¡
        mid = len(X_tensor) // 2
        data_batches = [X_tensor[:mid], X_tensor[mid:]]
        
        # è®­ç»ƒæ­¥éª¤
        losses = trainer.train_step(data_batches)
        
        # è®°å½•æŸå¤±
        training_history['epoch'].append(epoch)
        for key in ['total_loss', 'contrastive_loss', 'clustering_loss', 'consistency_loss']:
            loss_key = key.replace('_loss', '')
            training_history[key].append(losses[loss_key].item())
        
        # æ¯10è½®è¯„ä¼°ä¸€æ¬¡
        if (epoch + 1) % 10 == 0 or epoch == config.num_epochs - 1:
            eval_results = trainer.evaluate(X_tensor, y_tensor)
            
            ari = eval_results.get('adjusted_rand_score', 0)
            nmi = eval_results.get('normalized_mutual_info', 0)
            
            training_history['ari_score'].append(ari)
            training_history['nmi_score'].append(nmi)
            
            print(f"Epoch {epoch+1:3d}/{config.num_epochs}: "
                  f"Loss={losses['total']:.4f}, "
                  f"ARI={ari:.4f}, "
                  f"NMI={nmi:.4f}")
    
    # æœ€ç»ˆè¯„ä¼°
    print("\næœ€ç»ˆè¯„ä¼°...")
    final_results = trainer.evaluate(X_tensor, y_tensor)
    
    # è·å–èšç±»åˆ†é…
    cluster_assignments = trainer.get_cluster_assignments(X_tensor).numpy()
    embeddings = trainer.get_embeddings(X_tensor).numpy()
    
    # æ‰“å°ç»“æœ
    print("\næœ€ç»ˆç»“æœ:")
    for key, value in final_results.items():
        if isinstance(value, (int, float)):
            print(f"  {key}: {value:.4f}")
    
    return {
        'training_history': training_history,
        'final_results': final_results,
        'cluster_assignments': cluster_assignments,
        'embeddings': embeddings,
        'model': model
    }

def plot_training_curves(training_history, dataset_name, save_path=None):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    epochs = training_history['epoch']
    
    # æŸå¤±æ›²çº¿
    ax1.plot(epochs, training_history['total_loss'], 'b-', label='Total Loss')
    ax1.plot(epochs, training_history['contrastive_loss'], 'r-', label='Contrastive')
    ax1.plot(epochs, training_history['clustering_loss'], 'g-', label='Clustering')
    ax1.plot(epochs, training_history['consistency_loss'], 'm-', label='Consistency')
    ax1.set_title(f'{dataset_name} - è®­ç»ƒæŸå¤±')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # èšç±»æ€§èƒ½
    eval_epochs = list(range(10, len(training_history['ari_score']) * 10 + 1, 10))
    if len(eval_epochs) != len(training_history['ari_score']):
        eval_epochs = list(range(len(training_history['ari_score'])))
    
    ax2.plot(eval_epochs, training_history['ari_score'], 'o-', label='ARI Score')
    ax2.plot(eval_epochs, training_history['nmi_score'], 's-', label='NMI Score')
    ax2.set_title(f'{dataset_name} - èšç±»æ€§èƒ½')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Score')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # æŸå¤±åˆ†è§£ï¼ˆé¥¼å›¾ï¼‰
    final_losses = {
        'Contrastive': training_history['contrastive_loss'][-1],
        'Clustering': training_history['clustering_loss'][-1],
        'Consistency': training_history['consistency_loss'][-1]
    }
    
    ax3.pie(final_losses.values(), labels=final_losses.keys(), autopct='%1.1f%%')
    ax3.set_title(f'{dataset_name} - æœ€ç»ˆæŸå¤±åˆ†è§£')
    
    # æ€§èƒ½è¶‹åŠ¿
    if len(training_history['ari_score']) > 1:
        ax4.plot(eval_epochs, training_history['ari_score'], 'o-', color='blue', alpha=0.7)
        ax4.set_title(f'{dataset_name} - ARI Score è¶‹åŠ¿')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('ARI Score')
        ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path}")
    
    plt.show()

def run_federated_demo(X, y_true, dataset_name):
    """è¿è¡Œè”é‚¦å­¦ä¹ æ¼”ç¤º"""
    print(f"\n{'='*60}")
    print(f"è”é‚¦å­¦ä¹ æ¼”ç¤º - {dataset_name}")
    print(f"{'='*60}")
    
    # åˆ›å»ºè”é‚¦æ•°æ®åˆ†å‰²
    federated_data = OrchestraDataProcessor.create_federated_split(
        data=X,
        labels=y_true,
        num_parties=3,
        split_strategy='iid'
    )
    
    print("è”é‚¦æ•°æ®åˆ†å‰²:")
    for party, (data, labels) in federated_data.items():
        unique_labels = np.unique(labels)
        print(f"  {party}: {len(data)} æ ·æœ¬, {len(unique_labels)} ç±»åˆ«")
    
    return federated_data

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("Orchestra æ— ç›‘ç£è”é‚¦å­¦ä¹ æ¼”ç¤º")
    print("="*80)
    
    # æ£€æŸ¥ç¯å¢ƒ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = './demo_results'
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆ›å»ºåˆæˆæ•°æ®é›†
    print("\nåˆ›å»ºåˆæˆæ•°æ®é›†...")
    datasets = create_synthetic_datasets()
    
    # ä¸ºæ¯ä¸ªæ•°æ®é›†è¿è¡Œæ¼”ç¤º
    all_results = {}
    
    for dataset_key, (X, y_true, description) in datasets.items():
        print(f"\nå¤„ç†æ•°æ®é›†: {description}")
        
        # é…ç½®å‚æ•°
        num_clusters = len(np.unique(y_true))
        
        if dataset_key == 'high_dim':
            # é«˜ç»´æ•°æ®ä½¿ç”¨æ›´å¤§çš„ç½‘ç»œ
            config = OrchestraConfig(
                input_dim=X.shape[1],
                hidden_dims=[256, 128, 64],
                embedding_dim=32,
                num_clusters=num_clusters,
                num_epochs=50,
                batch_size=64,
                learning_rate=0.001
            )
        else:
            # 2Dæ•°æ®ä½¿ç”¨ç®€å•ç½‘ç»œ
            config = OrchestraConfig(
                input_dim=X.shape[1],
                hidden_dims=[64, 32],
                embedding_dim=16,
                num_clusters=num_clusters,
                num_epochs=30,
                batch_size=32,
                learning_rate=0.001
            )
        
        # è¿è¡ŒOrchestraèšç±»
        results = run_orchestra_clustering(X, y_true, description, config)
        all_results[dataset_key] = results
        
        # å¯è§†åŒ–ç»“æœï¼ˆä»…2Dæ•°æ®ï¼‰
        if X.shape[1] == 2:
            vis_path = os.path.join(output_dir, f'{dataset_key}_clustering.png')
            visualize_2d_data(X, y_true, results['cluster_assignments'], 
                            description, vis_path)
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        curve_path = os.path.join(output_dir, f'{dataset_key}_training.png')
        plot_training_curves(results['training_history'], description, curve_path)
        
        # è”é‚¦å­¦ä¹ æ¼”ç¤º
        federated_data = run_federated_demo(X, y_true, description)
    
    # æ€»ç»“ç»“æœ
    print("\n" + "="*80)
    print("æ¼”ç¤ºç»“æœæ€»ç»“")
    print("="*80)
    
    for dataset_key, results in all_results.items():
        dataset_name = datasets[dataset_key][2]
        final_results = results['final_results']
        
        print(f"\n{dataset_name}:")
        print(f"  ARI Score: {final_results.get('adjusted_rand_score', 0):.4f}")
        print(f"  NMI Score: {final_results.get('normalized_mutual_info', 0):.4f}")
        print(f"  Silhouette Score: {final_results.get('silhouette_score', 0):.4f}")
        print(f"  ä½¿ç”¨èšç±»æ•°: {final_results.get('num_clusters_used', 'N/A')}")
    
    print(f"\næ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    print("\nğŸ‰ Orchestraæ¼”ç¤ºå®Œæˆï¼")
    
    # ä½¿ç”¨å»ºè®®
    print("\n" + "="*80)
    print("ä¸‹ä¸€æ­¥å»ºè®®")
    print("="*80)
    print("1. è¿è¡Œå®Œæ•´CIFARå®éªŒ:")
    print("   python run_experiments.py --datasets cifar10 --num-epochs 20")
    print("\n2. è¿è¡ŒåŠŸèƒ½æµ‹è¯•:")
    print("   python test_orchestra.py")
    print("\n3. æŸ¥çœ‹è¯¦ç»†ä½¿ç”¨æŒ‡å—:")
    print("   æŸ¥çœ‹ setup_guide.md æ–‡ä»¶")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\næ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\næ¼”ç¤ºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()